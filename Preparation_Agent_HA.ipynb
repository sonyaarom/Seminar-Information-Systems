{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation Agent\n",
    "This notebook is dedicated to the data extraction and basic preparation done in the notebook 'Preparation Agent'. We start by installing dependencies that we need.\n",
    "As previously, Helper functions will be used. Available on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\anaconda3\\envs\\recomm\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "dir = 'D:/Master BWL HU/3. Semester/Seminar Information Systems/Seminar-Information-Systems-main'\n",
    "os.chdir(dir)\n",
    "\n",
    "from helper_functions import Helper\n",
    "from agents import Preparation_Agent\n",
    "import pandas as pd\n",
    "\n",
    "helper = Helper()\n",
    "\n",
    "dbfile  = \"D:/Master BWL HU/3. Semester/Seminar Information Systems/Seminar-Information-Systems-main/home-assistant_Chris_v3.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions will be used as in the previous works. Available at GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Data Pulling\n",
    "\n",
    "I removed the code for other databases as we will stick to the one called 'states'. In my own database I see states and attributes in different databases. \n",
    "<font color='red'>Check with Alona how she managed to put them together. And which version do we use? Did she send us updated data?\n",
    "</font> \n",
    "For now databases will be in the same form as in the first Alona's file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, initialization function contains also the function for SQL Pulling. <font color='red'> If you have any optimization ideas, please bring it in! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preparation_Agent:\n",
    "    import pandas as pd\n",
    "    helper = Helper()\n",
    "    def __init__(self, dbfile, shiftable_devices):\n",
    "        self.input = helper.export_sql(dbfile)\n",
    "        self.shiftable_devices = shiftable_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shiftable_devices = [\"sensor.shellyplug_s_4022d88961b4_power\", \"sensor.shellyplug_s_4022d88984b8_power\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>state</th>\n",
       "      <th>attributes</th>\n",
       "      <th>event_id</th>\n",
       "      <th>last_changed</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>old_state_id</th>\n",
       "      <th>attributes_id</th>\n",
       "      <th>context_id</th>\n",
       "      <th>context_user_id</th>\n",
       "      <th>context_parent_id</th>\n",
       "      <th>origin_idx</th>\n",
       "      <th>hash</th>\n",
       "      <th>shared_attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>update.home_assistant_supervisor_update</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.594923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>01GN5842Q23HBMB7EZ6KY9A13R</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2939789592</td>\n",
       "      <td>{\"auto_update\":true,\"installed_version\":\"2022....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>update.home_assistant_core_update</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.595605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>01GN5842Q3P07WA0KC0S4WMRZ4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2651871198</td>\n",
       "      <td>{\"auto_update\":false,\"installed_version\":\"2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>update.home_assistant_operating_system_update</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.596293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>01GN5842Q4738MTF23J7WJFNA8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2287136131</td>\n",
       "      <td>{\"auto_update\":false,\"installed_version\":\"9.4\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sun.sun</td>\n",
       "      <td>below_horizon</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.969050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>01GN58432SXZ8BV6NEGP0NDMQF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1454287034</td>\n",
       "      <td>{\"friendly_name\":\"Sun\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>zone.home</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.971450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>01GN58432VZ0634BF3S40SSSPA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1858961947</td>\n",
       "      <td>{\"latitude\":0,\"longitude\":0,\"radius\":100,\"pass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_id                                      entity_id          state  \\\n",
       "0         1        update.home_assistant_supervisor_update            off   \n",
       "1         2              update.home_assistant_core_update            off   \n",
       "2         3  update.home_assistant_operating_system_update            off   \n",
       "3         4                                        sun.sun  below_horizon   \n",
       "4         5                                      zone.home              0   \n",
       "\n",
       "  attributes event_id last_changed                last_updated  old_state_id  \\\n",
       "0       None     None         None  2022-12-25 18:18:34.594923           NaN   \n",
       "1       None     None         None  2022-12-25 18:18:34.595605           NaN   \n",
       "2       None     None         None  2022-12-25 18:18:34.596293           NaN   \n",
       "3       None     None         None  2022-12-25 18:18:34.969050           NaN   \n",
       "4       None     None         None  2022-12-25 18:18:34.971450           NaN   \n",
       "\n",
       "   attributes_id                  context_id context_user_id  \\\n",
       "0              1  01GN5842Q23HBMB7EZ6KY9A13R            None   \n",
       "1              2  01GN5842Q3P07WA0KC0S4WMRZ4            None   \n",
       "2              3  01GN5842Q4738MTF23J7WJFNA8            None   \n",
       "3              4  01GN58432SXZ8BV6NEGP0NDMQF            None   \n",
       "4              5  01GN58432VZ0634BF3S40SSSPA            None   \n",
       "\n",
       "  context_parent_id  origin_idx        hash  \\\n",
       "0              None           0  2939789592   \n",
       "1              None           0  2651871198   \n",
       "2              None           0  2287136131   \n",
       "3              None           0  1454287034   \n",
       "4              None           0  1858961947   \n",
       "\n",
       "                                   shared_attributes  \n",
       "0  {\"auto_update\":true,\"installed_version\":\"2022....  \n",
       "1  {\"auto_update\":false,\"installed_version\":\"2022...  \n",
       "2  {\"auto_update\":false,\"installed_version\":\"9.4\"...  \n",
       "3                            {\"friendly_name\":\"Sun\"}  \n",
       "4  {\"latitude\":0,\"longitude\":0,\"radius\":100,\"pass...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep = Preparation_Agent(dbfile, shiftable_devices)\n",
    "prep.input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"auto_update\":false,\"installed_version\":\"2022.12.8\",\"latest_version\":\"2022.12.8\",\"release_url\":\"https://www.home-assistant.io/latest-release-notes/\",\"skipped_version\":null,\"title\":\"Home Assistant Core\",\"friendly_name\":\"Home Assistant Core Update\"}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.input[\"shared_attributes\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Unpacking\n",
    "We received data frame that has dictionaries in column value. In this section, we bring the data frame to the wide format, additionally unpacking the column. After that I specify that I need values in kWh and choose nodes. <font color='red'>Note: user should specify the nodes. How does he do it? Is kWh okay?\n",
    "</font>\n",
    "\n",
    "Here I am dropping values but it is not sustainable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted for HA\n",
    "def unpacking_attributes(self, df):\n",
    "    output = df.copy()\n",
    "    output['shared_attributes']=output['shared_attributes'].apply(lambda x: x.replace('true','True'))\n",
    "    output['shared_attributes']=output['shared_attributes'].apply(lambda x: x.replace('false','False'))\n",
    "    output['shared_attributes']=output['shared_attributes'].apply(lambda x: x.replace('null','None'))\n",
    "\n",
    "    output['shared_attributes']=output['shared_attributes'].apply(lambda dat: dict(eval(dat)))\n",
    "    df2 = pd.json_normalize(output['shared_attributes'])\n",
    "    result = pd.DataFrame( pd.concat([output,df2], axis = 1).drop('shared_attributes', axis = 1))\n",
    "    result = result.dropna(axis = 1, thresh=int(0.95*(len(result.columns))))\n",
    "    return result\n",
    "\n",
    "setattr(Preparation_Agent, 'unpacking_attributes', unpacking_attributes)\n",
    "del unpacking_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unpacked  = prep.unpacking_attributes(prep.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>state</th>\n",
       "      <th>last_changed</th>\n",
       "      <th>last_updated</th>\n",
       "      <th>old_state_id</th>\n",
       "      <th>attributes_id</th>\n",
       "      <th>context_id</th>\n",
       "      <th>origin_idx</th>\n",
       "      <th>hash</th>\n",
       "      <th>...</th>\n",
       "      <th>popularity_thursday</th>\n",
       "      <th>popularity_friday</th>\n",
       "      <th>popularity_saturday</th>\n",
       "      <th>popularity_sunday</th>\n",
       "      <th>gps_accuracy</th>\n",
       "      <th>altitude</th>\n",
       "      <th>course</th>\n",
       "      <th>vertical_accuracy</th>\n",
       "      <th>source</th>\n",
       "      <th>visibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>update.home_assistant_supervisor_update</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.594923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>01GN5842Q23HBMB7EZ6KY9A13R</td>\n",
       "      <td>0</td>\n",
       "      <td>2939789592</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>update.home_assistant_core_update</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.595605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>01GN5842Q3P07WA0KC0S4WMRZ4</td>\n",
       "      <td>0</td>\n",
       "      <td>2651871198</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>update.home_assistant_operating_system_update</td>\n",
       "      <td>off</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.596293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>01GN5842Q4738MTF23J7WJFNA8</td>\n",
       "      <td>0</td>\n",
       "      <td>2287136131</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sun.sun</td>\n",
       "      <td>below_horizon</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.969050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>01GN58432SXZ8BV6NEGP0NDMQF</td>\n",
       "      <td>0</td>\n",
       "      <td>1454287034</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>zone.home</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-25 18:18:34.971450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>01GN58432VZ0634BF3S40SSSPA</td>\n",
       "      <td>0</td>\n",
       "      <td>1858961947</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48052</th>\n",
       "      <td>48053</td>\n",
       "      <td>sensor.bogenleuchte</td>\n",
       "      <td>3.64</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-09 16:04:03.304076</td>\n",
       "      <td>48047.0</td>\n",
       "      <td>3187</td>\n",
       "      <td>01GPBMCHK8NKZ9BDN58J2R4VR7</td>\n",
       "      <td>0</td>\n",
       "      <td>119982377</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48053</th>\n",
       "      <td>48054</td>\n",
       "      <td>sensor.olaf</td>\n",
       "      <td>3.02</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-09 16:04:04.365858</td>\n",
       "      <td>48048.0</td>\n",
       "      <td>3190</td>\n",
       "      <td>01GPBMCJMDFRDAMRBF5VH03ZH2</td>\n",
       "      <td>0</td>\n",
       "      <td>1243904514</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48054</th>\n",
       "      <td>48055</td>\n",
       "      <td>sensor.blub</td>\n",
       "      <td>3.02</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-09 16:04:07.163404</td>\n",
       "      <td>48049.0</td>\n",
       "      <td>3189</td>\n",
       "      <td>01GPBMCNBVQG0WZZBYZK2FPCRN</td>\n",
       "      <td>0</td>\n",
       "      <td>1905492863</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48055</th>\n",
       "      <td>48056</td>\n",
       "      <td>sensor.bogenleuchte</td>\n",
       "      <td>3.66</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-09 16:05:03.304884</td>\n",
       "      <td>48053.0</td>\n",
       "      <td>3187</td>\n",
       "      <td>01GPBMEC68DB8P98M3AQ9KNE4B</td>\n",
       "      <td>0</td>\n",
       "      <td>119982377</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48056</th>\n",
       "      <td>48057</td>\n",
       "      <td>sensor.olaf</td>\n",
       "      <td>3.04</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-09 16:05:04.366175</td>\n",
       "      <td>48054.0</td>\n",
       "      <td>3190</td>\n",
       "      <td>01GPBMED7EYAT90JSBVJWJEC2Q</td>\n",
       "      <td>0</td>\n",
       "      <td>1243904514</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48057 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_id                                      entity_id          state  \\\n",
       "0             1        update.home_assistant_supervisor_update            off   \n",
       "1             2              update.home_assistant_core_update            off   \n",
       "2             3  update.home_assistant_operating_system_update            off   \n",
       "3             4                                        sun.sun  below_horizon   \n",
       "4             5                                      zone.home              0   \n",
       "...         ...                                            ...            ...   \n",
       "48052     48053                            sensor.bogenleuchte           3.64   \n",
       "48053     48054                                    sensor.olaf           3.02   \n",
       "48054     48055                                    sensor.blub           3.02   \n",
       "48055     48056                            sensor.bogenleuchte           3.66   \n",
       "48056     48057                                    sensor.olaf           3.04   \n",
       "\n",
       "      last_changed                last_updated  old_state_id  attributes_id  \\\n",
       "0             None  2022-12-25 18:18:34.594923           NaN              1   \n",
       "1             None  2022-12-25 18:18:34.595605           NaN              2   \n",
       "2             None  2022-12-25 18:18:34.596293           NaN              3   \n",
       "3             None  2022-12-25 18:18:34.969050           NaN              4   \n",
       "4             None  2022-12-25 18:18:34.971450           NaN              5   \n",
       "...            ...                         ...           ...            ...   \n",
       "48052         None  2023-01-09 16:04:03.304076       48047.0           3187   \n",
       "48053         None  2023-01-09 16:04:04.365858       48048.0           3190   \n",
       "48054         None  2023-01-09 16:04:07.163404       48049.0           3189   \n",
       "48055         None  2023-01-09 16:05:03.304884       48053.0           3187   \n",
       "48056         None  2023-01-09 16:05:04.366175       48054.0           3190   \n",
       "\n",
       "                       context_id  origin_idx        hash  ...  \\\n",
       "0      01GN5842Q23HBMB7EZ6KY9A13R           0  2939789592  ...   \n",
       "1      01GN5842Q3P07WA0KC0S4WMRZ4           0  2651871198  ...   \n",
       "2      01GN5842Q4738MTF23J7WJFNA8           0  2287136131  ...   \n",
       "3      01GN58432SXZ8BV6NEGP0NDMQF           0  1454287034  ...   \n",
       "4      01GN58432VZ0634BF3S40SSSPA           0  1858961947  ...   \n",
       "...                           ...         ...         ...  ...   \n",
       "48052  01GPBMCHK8NKZ9BDN58J2R4VR7           0   119982377  ...   \n",
       "48053  01GPBMCJMDFRDAMRBF5VH03ZH2           0  1243904514  ...   \n",
       "48054  01GPBMCNBVQG0WZZBYZK2FPCRN           0  1905492863  ...   \n",
       "48055  01GPBMEC68DB8P98M3AQ9KNE4B           0   119982377  ...   \n",
       "48056  01GPBMED7EYAT90JSBVJWJEC2Q           0  1243904514  ...   \n",
       "\n",
       "      popularity_thursday popularity_friday popularity_saturday  \\\n",
       "0                     NaN               NaN                 NaN   \n",
       "1                     NaN               NaN                 NaN   \n",
       "2                     NaN               NaN                 NaN   \n",
       "3                     NaN               NaN                 NaN   \n",
       "4                     NaN               NaN                 NaN   \n",
       "...                   ...               ...                 ...   \n",
       "48052                 NaN               NaN                 NaN   \n",
       "48053                 NaN               NaN                 NaN   \n",
       "48054                 NaN               NaN                 NaN   \n",
       "48055                 NaN               NaN                 NaN   \n",
       "48056                 NaN               NaN                 NaN   \n",
       "\n",
       "      popularity_sunday gps_accuracy  altitude  course vertical_accuracy  \\\n",
       "0                   NaN          NaN       NaN     NaN               NaN   \n",
       "1                   NaN          NaN       NaN     NaN               NaN   \n",
       "2                   NaN          NaN       NaN     NaN               NaN   \n",
       "3                   NaN          NaN       NaN     NaN               NaN   \n",
       "4                   NaN          NaN       NaN     NaN               NaN   \n",
       "...                 ...          ...       ...     ...               ...   \n",
       "48052               NaN          NaN       NaN     NaN               NaN   \n",
       "48053               NaN          NaN       NaN     NaN               NaN   \n",
       "48054               NaN          NaN       NaN     NaN               NaN   \n",
       "48055               NaN          NaN       NaN     NaN               NaN   \n",
       "48056               NaN          NaN       NaN     NaN               NaN   \n",
       "\n",
       "      source visibility  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  \n",
       "...      ...        ...  \n",
       "48052    NaN        NaN  \n",
       "48053    NaN        NaN  \n",
       "48054    NaN        NaN  \n",
       "48055    NaN        NaN  \n",
       "48056    NaN        NaN  \n",
       "\n",
       "[48057 rows x 75 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unpacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes ID is not unique for one entity, therefore we use entity_id to identify the different sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unpacked.loc[data_unpacked['entity_id'] == \"sensor.shellyplug_s_4022d88984b8_power\"].attributes_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unpacked.unit_of_measurement.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "W_data = data_unpacked[data_unpacked.unit_of_measurement=='W']\n",
    "W_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(W_data[['last_updated', 'state']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_data = W_data.iloc[:,0:13]\n",
    "W_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_data.entity_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing only nodes in W\n",
    "Not optimally written code! However, this function only takes entities that have consumption in kWh and nodes defined by the user. <font color = 'red'> Next question is how to merge it with the home assistant so user can define which node to use? Also tried to get rid of NaN column, why it didn't work out? </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated\n",
    "def access_shiftable_devices(self, df, attrs= 'all'):\n",
    "    trial = df.copy()\n",
    "    trial.attributes_id = trial.attributes_id.dropna()\n",
    "    trial.state= pd.to_numeric(trial['state'], errors='coerce').dropna()\n",
    "    if attrs == 'all':\n",
    "        w_data = trial[trial.unit_of_measurement.isin(['W'])]\n",
    "        w_data = trial[trial.entity_id.isin(prep.shiftable_devices)]\n",
    "        #we can add entity_id, or disregard\n",
    "        w_data_long = w_data[['entity_id','last_updated','state']]\n",
    "        w_data_wide = pd.pivot(w_data_long,  index = ['last_updated'], columns = 'entity_id', values = 'state')\n",
    "    if attrs != 'all':\n",
    "        w_data = trial[trial.unit_of_measurement.isin(['W']) & trial.attributes_id.isin([attrs])]\n",
    "        w_data = trial[trial.entity_id.isin(prep.shiftable_devices)]\n",
    "        w_data_long = w_data[['entity_id','last_updated','state']]\n",
    "        w_data_wide = pd.pivot(w_data_long,  index = ['last_updated'], columns = 'entity_id', values = 'state')\n",
    "    result = w_data_wide.fillna(0).reset_index()\n",
    "    return(result)\n",
    "\n",
    "setattr(Preparation_Agent, 'access_shiftable_devices', access_shiftable_devices)\n",
    "del access_shiftable_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_w = prep.access_shiftable_devices(data_unpacked)\n",
    "data_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(data_w[['last_updated','sensor.shellyplug_s_4022d88984b8_power']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Data Preprocessing - Outlier Truncation\n",
    "Here comes <font color='red'> the first issue </font>, the consumption which was reported is really low. When we apply outlier truncation, all values that are not zero becoming zeros. Therefore, should we skip outlier truncation step?\n",
    "I am taking kWh - kilowatt hour. <font color='red'> Do you think I should take W better  </font>? Below one can see that only node 3 and 46 are active and activity is super low. I will create outlier truncation but I do not think that it should be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "data_w.hist(bins=10, figsize=(8,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewed data, no outliers <font color = 'red'> (as I personally believe) </font>. Data has no missing values. However, as I am not sure whether this will be the case with any data I will keep truncation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_truncation(self, series, factor=1.5, verbose=0):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    \n",
    "    lower_bound = q1 - factor*iqr\n",
    "    upper_bound = q3 + factor*iqr\n",
    "    \n",
    "    output = []\n",
    "    counter = 0\n",
    "    for item in (tqdm(series, desc=f'[outlier truncation: {series.name}]') if verbose != 0 else series):\n",
    "        if item > upper_bound:\n",
    "            output.append(int(upper_bound))\n",
    "            counter += 1\n",
    "        elif item < lower_bound:\n",
    "            output.append(int(lower_bound))\n",
    "            counter += 1\n",
    "        else:\n",
    "            output.append(item)\n",
    "    print(f'[outlier truncation: {series.name}]: {counter} outliers were truncated.') if verbose != 0 else None \n",
    "    return output\n",
    "\n",
    "# add the function to the class and delete global function\n",
    "setattr(Preparation_Agent, 'outlier_truncation', outlier_truncation)\n",
    "del outlier_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df['573 truncated'] = prep.outlier_truncation(data_w[\"sensor.shellyplug_s_4022d88984b8_power\"], verbose = 1)\n",
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncating multiple features\n",
    "def truncate(self, df, features='all', factor=1.5, verbose=0):\n",
    "    import time\n",
    "    output = df.copy()\n",
    "    features = df.select_dtypes(include=['int', 'float']).columns if features == 'all' else features\n",
    "\n",
    "    for feature in features:\n",
    "        time.sleep(0.2) if verbose != 0 else None\n",
    "        row_nn = df[feature] != 0                                                                  # truncate only the values for which the device uses energy\n",
    "        output.loc[row_nn, feature] = self.outlier_truncation(df.loc[row_nn, feature], factor=factor, verbose=verbose) # Truncatation factor = 1.5 * IQR\n",
    "        print('\\n') if verbose != 0 else None\n",
    "    return output\n",
    "\n",
    "# add to class\n",
    "setattr(Preparation_Agent, 'truncate', truncate)\n",
    "del truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_t = prep.truncate(data_w, verbose = 1)\n",
    "household_t.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(self, df, features='all', kind='MinMax', verbose=0):\n",
    "    output = df.copy()\n",
    "    features = df.select_dtypes(include=['int', 'float']).columns if features == 'all' else features\n",
    "    \n",
    "    if kind == 'MinMax':\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        output[features] = scaler.fit_transform(df[features])\n",
    "        print('[MinMaxScaler] Finished scaling the data.') if verbose != 0 else None\n",
    "    else:\n",
    "        raise InputError('Chosen scaling method is not available.')\n",
    "    return output \n",
    "\n",
    "# add to the class\n",
    "setattr(Preparation_Agent, 'scale', scale)\n",
    "del scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_s = prep.scale(household_t, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resampling by hour**\n",
    "First, I will create the function that aggregates the dataframe for the period X, it can be 60 minutes, it can be one day. It is resampled this way, that we receive sum of the variables per hour/day and not the average as in initial preparation agent. <font color = 'red'> If you consider that we should take mean, please let me know. </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(self, df, start, timedelta_params):\n",
    "    df.last_updated = pd.to_datetime(df.last_updated)\n",
    "    df = df.set_index('last_updated')\n",
    "    start = pd.to_datetime(start) if type(start) != type(pd.to_datetime('1970-01-01')) else start \n",
    "    end = start + pd.Timedelta(**timedelta_params)\n",
    "    return df[start:end].reset_index()\n",
    "\n",
    "# add to the helper class\n",
    "setattr(Helper, 'get_timespan', get_timespan)\n",
    "del get_timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data for one day\n",
    "helper = Helper()\n",
    "from datetime import datetime\n",
    "\n",
    "timedelta_params = {'days': 1, 'seconds': -1}   # returns data for just one day\n",
    "day = helper.get_timespan(data_w, \"2023-01-01\", timedelta_params)\n",
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregating per hour\n",
    "data = day.resample('60T', on = 'last_updated').mean()\n",
    "data.describe().round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'> Mean gives back NaN if there are only 0! So we have to replace all nan values with 0. Otherwise later on the load agent does not find right load patterns.</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = data_w.copy()\n",
    "output.last_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['last_updated'] = pd.to_datetime(output['last_updated'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.last_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.resample('60T', on=\"last_updated\").mean()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.replace(np.nan, 0)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'> In Zharova's approach there is function aggregate, I changed it to resampling. Note, she takes **mean**, I take sum as hourly total consumption </font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def aggregate(self, df, resample_param):\n",
    "#    return df.resample(resample_param).mean().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to helper and not to preparation agent\n",
    "def aggregate(self, df, period = '60T'):\n",
    "    output = df.copy()\n",
    "    output['last_updated'] = pd.to_datetime(output['last_updated'])\n",
    "    output = output.resample(period, on=\"last_updated\").mean()\n",
    "    output = output.replace(np.nan, 0)\n",
    "    return output\n",
    "\n",
    "setattr(Preparation_Agent, 'aggregate', aggregate)\n",
    "del aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prep.aggregate(data_w)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consumption(self, df, features='all', figsize='default', threshold=None, title='Consumption'):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df = df.copy()\n",
    "    features = [column for column in df.columns if column not in ['Unix', 'Issues']] if features == 'all' else features\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize) if figsize != 'default' else plt.subplots()\n",
    "    if threshold != None:\n",
    "        df['threshold'] = [threshold]*df.shape[0]\n",
    "        ax.plot(df['threshold'], color = 'tab:red')\n",
    "    for feature in features:\n",
    "        ax.plot(df[feature])\n",
    "    ax.legend(['threshold'] + features) if threshold != None else ax.legend(features)\n",
    "    ax.set_title(title);\n",
    "\n",
    "# add to the class\n",
    "setattr(Preparation_Agent, 'plot_consumption', plot_consumption)\n",
    "del plot_consumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **How do we define the threshold in our case?** </font> I randomly took threshold in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.plot_consumption(data, threshold = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Device Usage Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_usage(self, df, device, threshold):\n",
    "    return (df.loc[:, device] > threshold).astype('int')\n",
    "\n",
    "# add to class\n",
    "setattr(Preparation_Agent, 'get_device_usage', get_device_usage)\n",
    "del get_device_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(prep.get_device_usage(data, \"sensor.shellyplug_s_4022d88984b8_power\", 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.get_device_usage(data, \"sensor.shellyplug_s_4022d88961b4_power\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that creates binary variable and aggregates in order to show whether variable has been or not used in particular day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## daily appliance usage\n",
    "data_all = prep.aggregate(household_s, '1D')\n",
    "shiftable_devices = [\"sensor.shellyplug_s_4022d88961b4_power\", \"sensor.shellyplug_s_4022d88984b8_power\"]\n",
    "\n",
    "for appliance in shiftable_devices:\n",
    "    data_all['node'+ str(appliance) + '_usage'] = prep.get_device_usage(data_all, appliance, 0)\n",
    "data_all.filter(regex = 'usage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activity feature appear in my notebook a bit ealier than in original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity(self, df, active_appliances, threshold):\n",
    "    import pandas as pd\n",
    "    active = pd.DataFrame({appliance: df[appliance] > threshold for appliance in active_appliances})\n",
    "    return active.apply(any, axis = 1).astype('int')\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'get_activity', get_activity)\n",
    "del get_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shiftable_devices = [\"sensor.shellyplug_s_4022d88961b4_power\", \"sensor.shellyplug_s_4022d88984b8_power\"]\n",
    "\n",
    "data_all['activity'] = prep.get_activity(data_all, shiftable_devices, 0)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Periods since the last device usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_usage(self, series):\n",
    "    last_usage = []\n",
    "    for idx in range(len(series)):\n",
    "        shift = 1\n",
    "        if pd.isna(series.shift(periods = 1)[idx]):\n",
    "            shift = None\n",
    "        else:\n",
    "            while series.shift(periods = shift)[idx] == 0:\n",
    "                shift += 1\n",
    "        last_usage.append(shift)\n",
    "    return last_usage\n",
    "\n",
    "def get_last_usages(self, df, features):\n",
    "    import pandas as pd\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    for feature in features:\n",
    "        output['periods_since_last_'+str(feature)] = self.get_last_usage(df[feature])\n",
    "    output.set_index(df.index, inplace=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "setattr(Preparation_Agent, 'get_last_usage', get_last_usage)\n",
    "del get_last_usage\n",
    "\n",
    "setattr(Preparation_Agent, 'get_last_usages', get_last_usages)\n",
    "del get_last_usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_cols = data_all.filter(regex = 'usage')\n",
    "\n",
    "data_all = data_all.join(prep.get_last_usages(data_all, usage_cols))\n",
    "data_all.iloc[:, -10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Features Based on DatetimeIndex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_feature(self, df, features='all'):\n",
    "    import pandas as pd\n",
    "    functions = {\n",
    "        'hour': lambda df: df.index.hour, \n",
    "        'day_of_week': lambda df: df.index.dayofweek,\n",
    "        'day_name': lambda df: df.index.day_name().astype('category'),\n",
    "        'month': lambda df: df.index.month, \n",
    "        'month_name': lambda df: df.index.month_name().astype('category'),\n",
    "        'weekend': lambda df: [int(x in ['Saturday', 'Sunday']) for x in  list(df.index.day_name())]\n",
    "    }\n",
    "    if features == 'all':\n",
    "        output = pd.DataFrame({function[0]: function[1](df) for function in functions.items()})\n",
    "    else:\n",
    "        output = pd.DataFrame({function[0]: function[1](df) for function in functions.items() if function[0] in features})\n",
    "    output.set_index(df.index, inplace=True)\n",
    "    return output\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'get_time_feature', get_time_feature)\n",
    "del get_time_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = day.set_index('last_updated')\n",
    "day = day.loc[:,[\"sensor.shellyplug_s_4022d88961b4_power\", \"sensor.shellyplug_s_4022d88984b8_power\"]]\n",
    "day = day.reset_index()\n",
    "day['last_updated']=pd.to_datetime(day['last_updated'])\n",
    "day = day.set_index('last_updated')\n",
    "day = day.join(prep.get_time_feature(day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Lag Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_lags(self, df, features, lags):\n",
    "    import pandas as pd\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            output[f'{feature}_lag_{lag}'] = df[feature].shift(periods=lag)\n",
    "    return output\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'get_time_lags', get_time_lags)\n",
    "del get_time_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day['activity'] = prep.get_activity(day, shiftable_devices, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = day.join(prep.get_time_lags(day, ['hour','activity'], [1, 2, 5]))\n",
    "day.iloc[:, -6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimal threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_consumption(day, shiftable_devices, figsize=(18,10), threshold=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_threshold(self, df, threshold, appliances, figsize=(18,5)):\n",
    "    # data prep\n",
    "    for appliance in appliances:\n",
    "        df[str(appliance) + '_usage'] = self.get_device_usage(df, appliance, threshold)\n",
    "    df = df.join(self.get_time_feature(df))\n",
    "    df['activity'] = self.get_activity(df, appliances, threshold)\n",
    "\n",
    "    # plotting \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    usage_cols = [column for column in df.columns if str(column).endswith('_usage')]\n",
    "    columns = ['activity'] + usage_cols\n",
    "\n",
    "    fig, axes = plt.subplots(1,3, figsize=figsize)\n",
    "\n",
    "    # hour\n",
    "    hour = df.groupby('hour').mean()[columns]\n",
    "    hour.plot(ax=axes[0])\n",
    "    axes[0].set_ylim(-.1, 1.1);\n",
    "    axes[0].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per hour')\n",
    "\n",
    "    # week \n",
    "    usage_cols = [column for column in df.columns if str(column).endswith('_usage')]\n",
    "    week = df.groupby('day_name').mean()[columns]\n",
    "    week = week.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "    week.plot(ax=axes[1])\n",
    "    axes[1].set_ylim(-.1, 1.1);\n",
    "    #axes[1].set_xticklabels(['']+list(week.index), rotation=90)\n",
    "    axes[1].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per day of the week')\n",
    "\n",
    "    # month\n",
    "    usage_cols = [column for column in df.columns if str(column).endswith('_usage')]\n",
    "    month = df.groupby('month').mean()[columns]\n",
    "    month.plot(ax=axes[2])\n",
    "    axes[2].set_ylim(-.1, 1.1);\n",
    "    axes[2].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per month')\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'visualize_threshold', visualize_threshold)\n",
    "del visualize_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_thresholds(self, df, thresholds, appliances, figsize=(18,5)):\n",
    "    import time\n",
    "    from tqdm import tqdm\n",
    "    for threshold in tqdm(thresholds):\n",
    "        self.visualize_threshold(df, threshold, appliances, figsize)\n",
    "    time.sleep(0.2)\n",
    "    print('\\n')\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'validate_thresholds', validate_thresholds)\n",
    "del validate_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_s = household_s.set_index('last_updated')\n",
    "df = helper.aggregate(household_s, '7T')\n",
    "thresholds = [0] + list(np.geomspace(.1, .5, 5))\n",
    "\n",
    "prep.validate_thresholds(df, thresholds, shiftable_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .15\n",
    "\n",
    "prep.visualize_threshold(df, threshold, shiftable_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_reported(self, df):\n",
    "    return str(df.index.max())[:10]\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'last_reported', last_reported)\n",
    "del last_reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_data_tomorrow(self, df):\n",
    "    date = self.last_reported(df)\n",
    "    tomorrow = (pd.to_datetime(date) + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # determine how many hours we need to fill up (missing hours till new day + 23 for tomorrow, day of prediction)\n",
    "    hours_to_fill = 24 - int(str(df.index.max())[11:13]) + 23\n",
    "    \n",
    "    # add rows and fill up with dummy 0\n",
    "    for i in range(0,hours_to_fill):\n",
    "        idx = df.tail(1).index[0] + pd.Timedelta(hours=1)\n",
    "        df.loc[idx] = 0\n",
    "    return df\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'add_dummy_data_tomorrow', add_dummy_data_tomorrow)\n",
    "del add_dummy_data_tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preparation_Agent(dbfile, shiftable_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = prep.unpacking_attributes(helper.export_sql(dbfile))\n",
    "df = prep.access_shiftable_devices(df)\n",
    "df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "df = df.set_index('last_updated')\n",
    "# Aggregate to hour level\n",
    "df = helper.aggregate_load(df, '60T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep.add_dummy_data_tomorrow(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation_params = {\n",
    "    'features': 'all', \n",
    "    'factor': 1.5, \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "scale_params = {\n",
    "    'features': 'all', \n",
    "    'kind': 'MinMax', \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "aggregate_params = {\n",
    "    'resample_param': '60T'\n",
    "}\n",
    "#update with active appliances attributes_ids\n",
    "activity_params = {\n",
    "    'active_appliances': shiftable_devices,\n",
    "    'threshold': .15\n",
    "}\n",
    "\n",
    "time_params = {\n",
    "    'features': ['hour', 'day_name']\n",
    "}\n",
    "\n",
    "activity_lag_params = {\n",
    "    'features': ['activity'],\n",
    "    'lags': [24, 48, 72]\n",
    "}\n",
    "\n",
    "activity_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'aggregate': aggregate_params,\n",
    "    'activity': activity_params,\n",
    "    'time': time_params,\n",
    "    'activity_lag': activity_lag_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline activity\n",
    "def pipeline_activity(self, df, params):\n",
    "    from helper_functions import Helper\n",
    "    import pandas as pd\n",
    "    helper = Helper()\n",
    "    df = df.copy()\n",
    "    import pandas as pd\n",
    "    output = pd.DataFrame()\n",
    "\n",
    "    df  = self.unpacking_attributes(self.input)\n",
    "    df = self.access_shiftable_devices(df)\n",
    "    # Data cleaning\n",
    "    # df = self.truncate(df, **params['truncate'],)\n",
    "    # df = self.scale(df, **params['scale'])\n",
    "    # ignore scaling for now, we would just scale those variables, which does not make sense \n",
    "    # Index(['state_id', 'old_state_id', 'attributes_id', 'origin_idx', 'hash'], dtype='object')\n",
    "\n",
    "    df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "    df = df.set_index('last_updated')\n",
    "    # Aggregate to hour level\n",
    "    df = helper.aggregate_load(df, **params['aggregate'])\n",
    "    \n",
    "    # Add dummy data\n",
    "    df = self.add_dummy_data_tomorrow(df)\n",
    "\n",
    "    # Activity feature\n",
    "    output['activity'] = self.get_activity(df, **params['activity'])\n",
    "\n",
    "    ## Time feature\n",
    "    output = output.join(self.get_time_feature(df, **params['time']))\n",
    "\n",
    "    # Activity lags\n",
    "    output = output.join(self.get_time_lags(output, **params['activity_lag']))\n",
    "\n",
    "    # Dummy coding\n",
    "    output = pd.get_dummies(output, drop_first=True)\n",
    "\n",
    "    return output\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'pipeline_activity', pipeline_activity)\n",
    "del pipeline_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preparation_Agent(dbfile, shiftable_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.pipeline_activity(prep.input, activity_pipe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pipeline\n",
    "\n",
    "The load agent requires cleaned energy consumption data for the shiftable devices. Therefore, we calculate the device usage for these appliances based on the truncated and scaled data and return the energy consumption of the shiftable devices only if we detected a device usage. The data for the load agent will be aggregated to the hour level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation_params = {\n",
    "    'features': 'all', \n",
    "    'factor': 1.5, \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "scale_params = {\n",
    "    'features': 'all', \n",
    "    'kind': 'MinMax', \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "aggregate_params = {\n",
    "    'resample_param': '60T'\n",
    "}\n",
    "\n",
    "shiftable_devices = [\"sensor.shellyplug_s_4022d88961b4_power\", \"sensor.shellyplug_s_4022d88984b8_power\"]\n",
    "\n",
    "device_params = {\n",
    "    'threshold': 0.15\n",
    "}\n",
    "\n",
    "load_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'aggregate': aggregate_params,\n",
    "    'shiftable_devices': shiftable_devices, \n",
    "    'device': device_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline load\n",
    "def pipeline_load(self, df, params):\n",
    "    from helper_functions import Helper\n",
    "    import pandas as pd\n",
    "    helper = Helper()\n",
    "\n",
    "    df  = self.unpacking_attributes(self.input)\n",
    "    df = self.access_shiftable_devices(df)\n",
    "\n",
    "    df = df.copy()\n",
    "    output = pd.DataFrame()\n",
    "\n",
    "    # Data cleaning\n",
    "    # df = self.truncate(df, **params['truncate'],)\n",
    "    # scaled = self.scale(df, **params['scale'])\n",
    "    # ignore scaling for now, we would just scale those variables, which does not make sense \n",
    "    # Index(['state_id', 'old_state_id', 'attributes_id', 'origin_idx', 'hash'], dtype='object')\n",
    "    scaled = df.copy()\n",
    "\n",
    "    df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "    df = df.set_index('last_updated')\n",
    "\n",
    "    scaled['last_updated'] = pd.to_datetime(scaled['last_updated'])\n",
    "    scaled = scaled.set_index('last_updated')\n",
    "\n",
    "    # aggregate\n",
    "    df = helper.aggregate_load(df, **params['aggregate'])\n",
    "    scaled = helper.aggregate_load(scaled, **params['aggregate'])\n",
    "    \n",
    "    # add dummy data\n",
    "    df = self.add_dummy_data_tomorrow(df)\n",
    "\n",
    "    # Get device usage and transform to energy consumption\n",
    "    for device in params['shiftable_devices']:\n",
    "        df[str(device) + '_usage'] = self.get_device_usage(df, device, **params['device'])\n",
    "        output[device] = df.apply(lambda timestamp: timestamp[device] * timestamp[str(device) + '_usage'], axis = 1)\n",
    "\n",
    "    return output, scaled, df\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'pipeline_load', pipeline_load)\n",
    "del pipeline_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output, scaled, df = prep.pipeline_load(prep.input, load_pipe_params)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df['sensor.shellyplug_s_4022d88984b8_power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(output['sensor.shellyplug_s_4022d88984b8_power'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = \"red\"> Here, what should I do with NAs? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Pipeline\n",
    "\n",
    "We will use the same functionality from the activity pipeline to create the activity feature for the usage agent. Furthermore, we create the device usage features for the appliances for which we will provide recommendations. As appliances for which we will provide recommendations, we used appliances that require user interaction and are time-shiftable, i.e. not bound to any specific time during the day (e.g. Television, Computer, etc.). These appliances will be called â€œshiftable devicesâ€. Additionally, we create the periods since last usage feature for the shiftable devices, create time lags for activity and the shiftable devices, add further time features and aggregate the data to the day level. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation_params = {\n",
    "    'features': 'all', \n",
    "    'factor': 1.5, \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "scale_params = {\n",
    "    'features': 'all', \n",
    "    'kind': 'MinMax', \n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "aggregate_hour_params = {\n",
    "    'resample_param': '60T'\n",
    "}\n",
    "\n",
    "activity_params = {\n",
    "    'active_appliances': shiftable_devices,\n",
    "    'threshold': .1\n",
    "}\n",
    "\n",
    "shiftable_devices = [\"sensor.shellyplug_s_4022d88961b4_power\", \"sensor.shellyplug_s_4022d88984b8_power\"]\n",
    "\n",
    "device_params = {\n",
    "    'threshold': 0.15\n",
    "}\n",
    "\n",
    "aggregate_day_params = {\n",
    "    'resample_param': '1D'\n",
    "}\n",
    "\n",
    "time_params = {\n",
    "    'features': ['day_name', 'weekend']\n",
    "}\n",
    "\n",
    "usage_pipe_params = {\n",
    "    'truncate': truncation_params,\n",
    "    'scale': scale_params,\n",
    "    'aggregate_hour': aggregate_hour_params,\n",
    "    'activity': activity_params,\n",
    "    'aggregate_day': aggregate_day_params,\n",
    "    'shiftable_devices': shiftable_devices, \n",
    "    'device': device_params,\n",
    "    'time': time_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline usage\n",
    "def pipeline_usage(self, df, params):\n",
    "    from helper_functions import Helper\n",
    "    import pandas as pd\n",
    "\n",
    "    helper = Helper()\n",
    "\n",
    "    df  = self.unpacking_attributes(self.input)\n",
    "    df = self.access_shiftable_devices(df)\n",
    "\n",
    "    df = df.copy()\n",
    "    output = pd.DataFrame()\n",
    "\n",
    "    # Data cleaning\n",
    "    # df = self.truncate(df, **params['truncate'],)\n",
    "    # scaled = self.scale(df, **params['scale'])\n",
    "    # ignore scaling for now, we would just scale those variables, which does not make sense \n",
    "    # Index(['state_id', 'old_state_id', 'attributes_id', 'origin_idx', 'hash'], dtype='object')\n",
    "    scaled = df.copy()\n",
    "\n",
    "    # df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "    # df = df.set_index('last_updated')\n",
    "    scaled['last_updated'] = pd.to_datetime(scaled['last_updated'])\n",
    "    scaled = scaled.set_index('last_updated')\n",
    "\n",
    "    # Aggregate to hour level\n",
    "    scaled = helper.aggregate_load(scaled, **params['aggregate_hour'])\n",
    "    \n",
    "    # Add dummy data\n",
    "    scaled = self.add_dummy_data_tomorrow(scaled)\n",
    "\n",
    "    # Activity feature\n",
    "    output['activity'] = self.get_activity(scaled, **params['activity'])\n",
    "\n",
    "    # Get device usage and transform to energy consumption\n",
    "    for device in params['shiftable_devices']:\n",
    "        output[str(device) + '_usage'] = self.get_device_usage(scaled, device, **params['device'])\n",
    "\n",
    "    # aggregate and convert from mean to binary\n",
    "    output = helper.aggregate(output, **params['aggregate_day'])\n",
    "    output = output.apply(lambda x: (x > 0).astype('int'))\n",
    "\n",
    "    # Last usage\n",
    "    output = output.join(self.get_last_usages(output, output.columns))\n",
    "\n",
    "    # Time features\n",
    "    output = output.join(self.get_time_feature(output, **params['time']))\n",
    "\n",
    "    # lags\n",
    "    output = output.join(self.get_time_lags(output, ['activity'] + [str(device)+'_usage' for device in params['shiftable_devices']], [1,2,3]))\n",
    "    output['active_last_2_days'] = ((output.activity_lag_1 == 1) | (output.activity_lag_2 == 1)).astype('int')\n",
    "\n",
    "    # dummy coding\n",
    "    output = pd.get_dummies(output, drop_first=True)\n",
    "    return output\n",
    "\n",
    "# add to the Preparation_agent class\n",
    "setattr(Preparation_Agent, 'pipeline_usage', pipeline_usage)\n",
    "del pipeline_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = prep.pipeline_usage(prep.input, usage_pipe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final: Complete Preparation Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# preparation agent ###############################################################################\n",
    "###################################################################################################\n",
    "class Preparation_Agent:\n",
    "#installing dependencies\n",
    "    import pandas as pd\n",
    "    #uploading data and simple data wrangling\n",
    "    def __init__(self, dbfile, shiftable_devices):\n",
    "        from helper_functions import Helper\n",
    "        helper = Helper()\n",
    "        self.input = helper.export_sql(dbfile)\n",
    "        self.shiftable_devices = shiftable_devices\n",
    "\n",
    "    def unpacking_attributes(self, df):\n",
    "        import pandas as pd\n",
    "        output = df.copy()\n",
    "        output['shared_attributes']=output['shared_attributes'].apply(lambda x: x.replace('true','True'))\n",
    "        output['shared_attributes']=output['shared_attributes'].apply(lambda x: x.replace('false','False'))\n",
    "        output['shared_attributes']=output['shared_attributes'].apply(lambda x: x.replace('null','None'))\n",
    "\n",
    "        output['shared_attributes']=output['shared_attributes'].apply(lambda dat: dict(eval(dat)))\n",
    "        df2 = pd.json_normalize(output['shared_attributes'])\n",
    "        result = pd.DataFrame( pd.concat([output,df2], axis = 1).drop('shared_attributes', axis = 1))\n",
    "        result = result.dropna(axis = 1, thresh=int(0.95*(len(result.columns))))\n",
    "        return result\n",
    "\n",
    "    def access_shiftable_devices(self, df, attrs= 'all'):\n",
    "        import pandas as pd\n",
    "        trial = df.copy()\n",
    "        trial.attributes_id = trial.attributes_id.dropna()\n",
    "        trial.state= pd.to_numeric(trial['state'], errors='coerce').dropna()\n",
    "        if attrs == 'all':\n",
    "            w_data = trial[trial.unit_of_measurement.isin(['W'])]\n",
    "            w_data = trial[trial.entity_id.isin(self.shiftable_devices)]\n",
    "            w_data_long = w_data[['entity_id','last_updated','state']]\n",
    "            w_data_wide = pd.pivot(w_data_long,  index = ['last_updated'], columns = 'entity_id', values = 'state')\n",
    "        if attrs != 'all':\n",
    "            w_data = trial[trial.unit_of_measurement.isin(['W']) & trial.attributes_id.isin([attrs])]\n",
    "            w_data = trial[trial.entity_id.isin(self.shiftable_devices)]\n",
    "            w_data_long = w_data[['entity_id','last_updated','state']]\n",
    "            w_data_wide = pd.pivot(w_data_long,  index = ['last_updated'], columns = 'entity_id', values = 'state')\n",
    "        result = w_data_wide.fillna(0).reset_index()\n",
    "        return(result)\n",
    "    \n",
    "    #basic preprocessing\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def outlier_truncation(self, series, factor=1.5, verbose=0):\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        q1 = series.quantile(0.25)\n",
    "        q3 = series.quantile(0.75)\n",
    "        iqr = q3-q1\n",
    "        \n",
    "        lower_bound = q1 - factor*iqr\n",
    "        upper_bound = q3 + factor*iqr\n",
    "        \n",
    "        output = []\n",
    "        counter = 0\n",
    "        for item in (tqdm(series, desc=f'[outlier truncation: {series.name}]') if verbose != 0 else series):\n",
    "            if item > upper_bound:\n",
    "                output.append(int(upper_bound))\n",
    "                counter += 1\n",
    "            elif item < lower_bound:\n",
    "                output.append(int(lower_bound))\n",
    "                counter += 1\n",
    "            else:\n",
    "                output.append(item)\n",
    "        print(f'[outlier truncation: {series.name}]: {counter} outliers were truncated.') if verbose != 0 else None \n",
    "        return output\n",
    "    \n",
    "    def scale(self, df, features='all', kind='MinMax', verbose=0):\n",
    "        output = df.copy()\n",
    "        features = output.select_dtypes(include=['int', 'float']).columns if features == 'all' else features\n",
    "\n",
    "        if kind == 'MinMax':\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            \n",
    "            scaler = MinMaxScaler()\n",
    "            output[features] = scaler.fit_transform(output[features])\n",
    "            print('[MinMaxScaler] Finished scaling the data.') if verbose != 0 else None\n",
    "        else:\n",
    "            raise InputError('Chosen scaling method is not available.')\n",
    "        return output \n",
    "\n",
    "    def get_timespan(self, df, start, timedelta_params):\n",
    "        df.last_updated = pd.to_datetime(df.last_updated)\n",
    "        df = df.set_index('last_updated')\n",
    "        start = pd.to_datetime(start) if type(start) != type(pd.to_datetime('1970-01-01')) else start \n",
    "        end = start + pd.Timedelta(**timedelta_params)\n",
    "        return df[start:end].reset_index()\n",
    "    \n",
    "    def truncate(self, df, features='all', factor=1.5, verbose=0):\n",
    "        import time\n",
    "        output = df.copy()\n",
    "        features = df.select_dtypes(include=['int', 'float']).columns if features == 'all' else features\n",
    "\n",
    "        for feature in features:\n",
    "            time.sleep(0.2) if verbose != 0 else None\n",
    "            row_nn = df[feature] != 0                                                                  # truncate only the values for which the device uses energy\n",
    "            output.loc[row_nn, feature] = self.outlier_truncation(df.loc[row_nn, feature], factor=factor, verbose=verbose) # Truncatation factor = 1.5 * IQR\n",
    "            print('\\n') if verbose != 0 else None\n",
    "        return output\n",
    "    \n",
    "    def last_reported(self, df):\n",
    "        return str(df.index.max())[:10]\n",
    "    \n",
    "    def add_dummy_data_tomorrow(self, df):\n",
    "        date = self.last_reported(df)\n",
    "        tomorrow = (pd.to_datetime(date) + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # determine how many hours we need to fill up (missing hours till new day + 23 for tomorrow, day of prediction)\n",
    "        hours_to_fill = 24 - int(str(df.index.max())[11:13]) + 23\n",
    "\n",
    "        # add rows and fill up with dummy 0\n",
    "        for i in range(0,hours_to_fill):\n",
    "            idx = df.tail(1).index[0] + pd.Timedelta(hours=1)\n",
    "            df.loc[idx] = 0\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def plot_consumption(self, df, features='all', figsize='default', threshold=None, title='Consumption'):\n",
    "        df = df.copy()\n",
    "        features = [column for column in df.columns if column not in ['Unix', 'Issues']] if features == 'all' else features\n",
    "        fig, ax = plt.subplots(figsize=figsize) if figsize != 'default' else plt.subplots()\n",
    "        if threshold != None:\n",
    "            df['threshold'] = [threshold]*df.shape[0]\n",
    "            ax.plot(df['threshold'], color = 'tab:red')\n",
    "        for feature in features:\n",
    "            ax.plot(df[feature])\n",
    "        ax.legend(['threshold'] + features) if threshold != None else ax.legend(features)\n",
    "        ax.set_title(title);\n",
    "    # feature creation\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    def get_device_usage(self, df, device, threshold):\n",
    "        return (df.loc[:, device] > threshold).astype('int')\n",
    "\n",
    "    def get_activity(self, df, active_appliances, threshold):\n",
    "        import pandas as pd\n",
    "        active = pd.DataFrame({appliance: df[appliance] > threshold for appliance in active_appliances})\n",
    "        return active.apply(any, axis = 1).astype('int')\n",
    "\n",
    "    def get_last_usage(self, series):\n",
    "        import pandas as pd\n",
    "        last_usage = []\n",
    "        for idx in range(len(series)):\n",
    "            shift = 1\n",
    "            if pd.isna(series.shift(periods = 1)[idx]):\n",
    "                shift = None\n",
    "            else:\n",
    "                while series.shift(periods = shift)[idx] == 0:\n",
    "                    shift += 1\n",
    "            last_usage.append(shift)\n",
    "        return last_usage\n",
    "\n",
    "    def get_last_usages(self, df, features):\n",
    "        import pandas as pd\n",
    "\n",
    "        output = pd.DataFrame()\n",
    "        for feature in features:\n",
    "            output['periods_since_last_'+str(feature)] = self.get_last_usage(df[feature])\n",
    "        output.set_index(df.index, inplace=True)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def get_time_feature(self, df, features='all'):\n",
    "        import pandas as pd\n",
    "        functions = {\n",
    "            'hour': lambda df: df.index.hour, \n",
    "            'day_of_week': lambda df: df.index.dayofweek,\n",
    "            'day_name': lambda df: df.index.day_name().astype('category'),\n",
    "            'month': lambda df: df.index.month, \n",
    "            'month_name': lambda df: df.index.month_name().astype('category'),\n",
    "            'weekend': lambda df: [int(x in ['Saturday', 'Sunday']) for x in  list(df.index.day_name())]\n",
    "        }\n",
    "        if features == 'all':\n",
    "            output = pd.DataFrame({function[0]: function[1](df) for function in functions.items()})\n",
    "        else:\n",
    "            output = pd.DataFrame({function[0]: function[1](df) for function in functions.items() if function[0] in features})\n",
    "        output.set_index(df.index, inplace=True)\n",
    "        return output\n",
    "    \n",
    "    def get_time_lags(self, df, features, lags):\n",
    "        import pandas as pd\n",
    "        output = pd.DataFrame()\n",
    "        for feature in features:\n",
    "            for lag in lags:\n",
    "                output[f'{feature}_lag_{lag}'] = df[feature].shift(periods=lag)\n",
    "        return output\n",
    "\n",
    "    #visualising threshold\n",
    "    # ------------------------------------------------------------------------------------------- \n",
    "    def visualize_threshold(self, df, threshold, appliances, figsize=(18,5)):\n",
    "        import pandas as pd\n",
    "        # data prep\n",
    "        for appliance in appliances:\n",
    "            df[str(appliance) + '_usage'] = self.get_device_usage(df, appliance, threshold)\n",
    "        df = df.join(self.get_time_feature(df))\n",
    "        df['activity'] = self.get_activity(df, appliances, threshold)\n",
    "\n",
    "        # plotting \n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        usage_cols = [column for column in df.columns if str(column).endswith('_usage')]\n",
    "        columns = ['activity'] + usage_cols\n",
    "\n",
    "        fig, axes = plt.subplots(1,3, figsize=figsize)\n",
    "\n",
    "        # hour\n",
    "        hour = df.groupby('hour').mean()[columns]\n",
    "        hour.plot(ax=axes[0])\n",
    "        axes[0].set_ylim(-.1, 1.1);\n",
    "        axes[0].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per hour')\n",
    "\n",
    "        # week \n",
    "        usage_cols = [column for column in df.columns if str(column).endswith('_usage')]\n",
    "        week = df.groupby('day_name').mean()[columns]\n",
    "        week = week.reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "        week.plot(ax=axes[1])\n",
    "        axes[1].set_ylim(-.1, 1.1);\n",
    "        #axes[1].set_xticklabels(['']+list(week.index), rotation=90)\n",
    "        axes[1].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per day of the week')\n",
    "\n",
    "        # month\n",
    "        usage_cols = [column for column in df.columns if str(column).endswith('_usage')]\n",
    "        month = df.groupby('month').mean()[columns]\n",
    "        month.plot(ax=axes[2])\n",
    "        axes[2].set_ylim(-.1, 1.1);\n",
    "        axes[2].set_title(f'[threshold: {round(threshold, 4)}] Activity ratio per month')\n",
    "    def validate_thresholds(self, df, thresholds, appliances, figsize=(18,5)):\n",
    "\n",
    "        for threshold in tqdm(thresholds):\n",
    "            self.visualize_threshold(df, threshold, appliances, figsize)\n",
    "        time.sleep(0.2)\n",
    "        print('\\n')\n",
    "    \n",
    "    #pipelines\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    #pipeline load\n",
    "    def pipeline_load(self, df, params):\n",
    "        from helper_functions import Helper\n",
    "        import pandas as pd\n",
    "        helper = Helper()\n",
    "        \n",
    "        df  = self.unpacking_attributes(self.input)\n",
    "        df = self.access_shiftable_devices(df)\n",
    "        \n",
    "        df = df.copy()\n",
    "        output = pd.DataFrame()\n",
    "\n",
    "        # Data cleaning\n",
    "        # df = self.truncate(df, **params['truncate'],)\n",
    "        # scaled = self.scale(df, **params['scale'])\n",
    "        # ignore scaling for now, we would just scale those variables, which does not make sense \n",
    "        # Index(['state_id', 'old_state_id', 'attributes_id', 'origin_idx', 'hash'], dtype='object')\n",
    "        scaled = df.copy()\n",
    "\n",
    "        df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "        df = df.set_index('last_updated')\n",
    "\n",
    "        scaled['last_updated'] = pd.to_datetime(scaled['last_updated'])\n",
    "        scaled = scaled.set_index('last_updated')\n",
    "\n",
    "        # aggregate\n",
    "        df = helper.aggregate_load(df, **params['aggregate'])\n",
    "        scaled = helper.aggregate_load(scaled, **params['aggregate'])\n",
    "        \n",
    "        # Add dummy data\n",
    "        df = self.add_dummy_data_tomorrow(df)\n",
    "        scaled = self.add_dummy_data_tomorrow(scaled)\n",
    "\n",
    "        # Get device usage and transform to energy consumption\n",
    "        for device in params['shiftable_devices']:\n",
    "            df[str(device) + '_usage'] = self.get_device_usage(df, device, **params['device'])\n",
    "            output[device] = df.apply(lambda timestamp: timestamp[device] * timestamp[str(device) + '_usage'], axis = 1)\n",
    "\n",
    "        return output, scaled, df\n",
    "    #pipeline usage\n",
    "    def pipeline_usage(self, df, params):\n",
    "        from helper_functions import Helper\n",
    "        import pandas as pd\n",
    "\n",
    "        helper = Helper()\n",
    "\n",
    "        df  = self.unpacking_attributes(self.input)\n",
    "        df = self.access_shiftable_devices(df)\n",
    "        \n",
    "        df = df.copy()\n",
    "        output = pd.DataFrame()\n",
    "\n",
    "        # Data cleaning\n",
    "        # df = self.truncate(df, **params['truncate'],)\n",
    "        # scaled = self.scale(df, **params['scale'])\n",
    "        # ignore scaling for now, we would just scale those variables, which does not make sense \n",
    "        # Index(['state_id', 'old_state_id', 'attributes_id', 'origin_idx', 'hash'], dtype='object')\n",
    "        scaled = df.copy()\n",
    "        \n",
    "        # df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "        # df = df.set_index('last_updated')\n",
    "        scaled['last_updated'] = pd.to_datetime(scaled['last_updated'])\n",
    "        scaled = scaled.set_index('last_updated')\n",
    "        \n",
    "        # Aggregate to hour level\n",
    "        scaled = helper.aggregate_load(scaled, **params['aggregate_hour'])\n",
    "        \n",
    "        # Add dummy data\n",
    "        scaled = self.add_dummy_data_tomorrow(scaled)\n",
    "\n",
    "\n",
    "        # Activity feature\n",
    "        output['activity'] = self.get_activity(scaled, **params['activity'])\n",
    "\n",
    "        # Get device usage and transform to energy consumption\n",
    "        for device in params['shiftable_devices']:\n",
    "            output[str(device) + '_usage'] = self.get_device_usage(scaled, device, **params['device'])\n",
    "\n",
    "        # aggregate and convert from mean to binary\n",
    "        output = helper.aggregate(output, **params['aggregate_day'])\n",
    "        output = output.apply(lambda x: (x > 0).astype('int'))\n",
    "\n",
    "        # Last usage\n",
    "        output = output.join(self.get_last_usages(output, output.columns))\n",
    "        \n",
    "        # Time features\n",
    "        output = output.join(self.get_time_feature(output, **params['time']))\n",
    "\n",
    "        # lags\n",
    "        output = output.join(self.get_time_lags(output, ['activity'] + [str(device)+'_usage' for device in params['shiftable_devices']], [1,2,3]))\n",
    "        output['active_last_2_days'] = ((output.activity_lag_1 == 1) | (output.activity_lag_2 == 1)).astype('int')\n",
    "\n",
    "        # dummy coding\n",
    "        output = pd.get_dummies(output, drop_first=True)\n",
    "        return output\n",
    "\n",
    "    #pipeline activity\n",
    "    def pipeline_activity(self, df, params):\n",
    "        from helper_functions import Helper\n",
    "        import pandas as pd\n",
    "        helper = Helper()\n",
    "        df = df.copy()\n",
    "        import pandas as pd\n",
    "        output = pd.DataFrame()\n",
    "\n",
    "        df  = self.unpacking_attributes(self.input)\n",
    "        df = self.access_shiftable_devices(df)\n",
    "        # Data cleaning\n",
    "        # df = self.truncate(df, **params['truncate'],)\n",
    "        # df = self.scale(df, **params['scale'])\n",
    "        # ignore scaling for now, we would just scale those variables, which does not make sense \n",
    "        # Index(['state_id', 'old_state_id', 'attributes_id', 'origin_idx', 'hash'], dtype='object')\n",
    "\n",
    "        df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "        df = df.set_index('last_updated')\n",
    "        # Aggregate to hour level\n",
    "        df = helper.aggregate_load(df, **params['aggregate'])\n",
    "        \n",
    "        # Add dummy data\n",
    "        df = self.add_dummy_data_tomorrow(df)\n",
    "\n",
    "        # Activity feature\n",
    "        output['activity'] = self.get_activity(df, **params['activity'])\n",
    "        \n",
    "        ## Time feature\n",
    "        output = output.join(self.get_time_feature(df, **params['time']))\n",
    "\n",
    "        # Activity lags\n",
    "        output = output.join(self.get_time_lags(output, **params['activity_lag']))\n",
    "\n",
    "        # Dummy coding\n",
    "        output = pd.get_dummies(output, drop_first=True)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prep = Preparation_Agent(helper.export_sql(dbfile), shiftable_devices)\n",
    "df = prep.pipeline_activity(helper.export_sql(dbfile), activity_pipe_params)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "263.991px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "fd19f899bcdfa6c353e9525ef244de6eb28a54f3ba596d530144acef4e3bc685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
